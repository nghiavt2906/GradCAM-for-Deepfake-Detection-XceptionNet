![brave_XzVcPn8XkD](https://github.com/user-attachments/assets/4601c083-9a88-475d-b6e9-1b8e25ddf376)

## GradCAM as Black-box Explainability method for Deepfake Detection CNN models (XceptionNet)
### Overview
#### Assignment 1 for COMP-6936 (Advanced Machine Learning)
This repository demonstrates applying Explainable AI method like `Grad-CAM` to a CNN model, `XceptionNet`, for Deepfake Detection task. By employing `Grad-CAM`, it helps to give more insights and justify on how decisions are made by CNN models.

### Pretrained model used:
- `XceptionNet`: the model has been trained on dataset from [Celeb-DF: A Large-scale Challenging Dataset for DeepFake Forensics](https://github.com/yuezunli/celeb-deepfakeforensics), then it was used to predict on new dataset from [Kaggle Deepfake Detection Challenge](https://www.kaggle.com/competitions/deepfake-detection-challenge/data) which it has never seen before. Thus, the results can help to draw conclusion on how well the model can generalize on unseen data.

### Methodology
A pretrained `XceptionNet` model was employed to make predictions on new dataset for Deepfake Detection task and applied with Grad-CAM to explain how the model made decisions based on important parts on images. Below are the following steps for the approach:
1. Load and process videos using `OpenCV` library
2. Crop images with face landmarks using `dlib`. Some videos were removed from the dataset due to having too low-light conditions to recognize the faces. Thus, the total number of videos to proceed for prediction step was 381 videos.
3. Load pretrained `XceptionNet` model. The pretrained weights were loaded from this [Github repository](https://github.com/vikrampande7/deepfake-detection). Some of the code for processing images dataset were also taken from this repo for reference.
4. Run the model to make predictions on the processed images. A video is classified as `REAL` (0) or `FAKE` (1) based on a predefined condition. If the percentage of the frames of that video predicted as `FAKE` (1) is greater than a certain `threshold`, which is a hyperparameter, then it is generated by Deepfake. Otherwise, it is classified as `REAL` or not generated by Deepfake. The `threshold` used in this experiment was set to `0.5`.
5. Generate and analyze evaluation metrics regarding predictions from the pretrained model.
6. `timm-vis` library was used to run Grad-CAM and visualize the results using `matplotlib`.
#### File Description
- `ASM1.ipynb`: contains the code for running model predictions and generating Grad-CAM
### Evaluation Metrics
![brave_M4ZkFoLc21](https://github.com/user-attachments/assets/13df1dae-9fcc-4e2d-bcf0-76881b12ccbf)
![brave_J7J9i3xrdl](https://github.com/user-attachments/assets/cd7eaf08-1552-4ed2-8e7a-ace91cde7287)

### Results from Grad-CAM
![brave_PYy9Z3E85n](https://github.com/user-attachments/assets/6569a453-7022-4e83-9f97-7df3c53ac753)

### Conclusion
From the evaluation metrics and Grad-CAM images, We can see that the pretrained model failed to recognize most of the videos generated by Deepfake while only 2 `REAL` videos were misclassified as `FAKE`. This can be explained due to videos from Celeb-DF dataset being generated using a different method other than the method used to generate videos from Kaggle Challenge. By employing Grad-CAM, it can be explained and justified how the pretrained model recognizes certain parts of an image of a person's face, usually around the chin, the upper left part of a face (i.e. left eye, left ear) and around the nose.
### References
[1] Deepfake-detection by vikrampande7. https://github.com/vikrampande7/deepfake-detection

[2] timm-vis: PyTorch Image Models Visualizer. https://github.com/novice03/timm-vis

[3] Deepfake Detection Challenge. https://www.kaggle.com/competitions/deepfake-detection-challenge

[4] Celeb-DF: A Large-scale Challenging Dataset for DeepFake Forensics. https://github.com/yuezunli/celeb-deepfakeforensics
